{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac535cf-32e4-4c01-a6a2-ffc82f3828ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:26:01.655168Z",
     "start_time": "2025-07-01T09:26:01.201782Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb67056-ac43-41fb-9c4a-ac5168ad822b",
   "metadata": {},
   "source": [
    "## YOLO\n",
    "\n",
    "In this section, we will explore one of the state-of-the-art (SOTA) models in Machine Learning.\n",
    "\n",
    "**You Only Look Once: Unified, Real-Time Object Detection**  \n",
    "[Article](https://arxiv.org/pdf/1506.02640)  \n",
    "[Presentation](https://www.youtube.com/watch?v=NM6lrxy0bxs&pp=ygUkeW91IG9ubHkgbG9vayBvbmNlIHByZXNlbnRhdGlvbiBjdnBy)\n",
    "\n",
    "YOLO (You Only Look Once) is a deep neural network architecture initially designed for real-time object detection in images. Classical detectors such as R-CNN, Fast R-CNN, and Faster R-CNN operate in several phases:\n",
    "\n",
    "1. **Region proposal generation** using a Region Proposal Network (RPN).\n",
    "2. **Feature extraction** for each proposed region.\n",
    "3. **Classification and bounding box refinement** for the detected objects.\n",
    "\n",
    "Although these methods can achieve high accuracy, they are computationally expensive and often unsuitable for real-time applications.\n",
    "\n",
    "YOLO follows a unified approach: it divides the image into a grid and processes each cell simultaneously to predict both the bounding boxes and the object classes.This integration enables YOLO to achieve notable speed without significantly compromising accuracy. Furthermore, thanks to its optimized design, YOLO has been modified and adapted to perform a wide range of computer vision tasks. It is currently capable of classification, detection, segmentation, object tracking in video, and body motion tracking.\n",
    "\n",
    "This approach offers several advantages:\n",
    "\n",
    "- **Very high speed**, making it suitable for real-time applications.\n",
    "- **Simple architecture**, easy to train and deploy.\n",
    "- **Global reasoning**: the model sees the entire image when making predictions, improving contextual understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bd7aa-cae8-45ce-8d96-fb3163106f57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evolution of YOLO\n",
    "\n",
    "YOLO (You Only Look Once) has undergone significant development since its first version in 2015. Below is a summary of the major milestones in its evolution:\n",
    "\n",
    "### YOLO (2015)\n",
    "- Introduced in the paper *\"You Only Look Once: Unified, Real-Time Object Detection\"* by Redmon et al.\n",
    "- Proposed a unified model for object detection that treats detection as a regression problem.\n",
    "- Divides the image into an S×S grid and predicts bounding boxes and class probabilities for each grid cell.\n",
    "- Very fast but suffered from localization errors and struggled with detecting small objects.\n",
    "\n",
    "### YOLOv2 (YOLO9000, 2016)\n",
    "- Improved accuracy and speed using a better backbone network (Darknet-19).\n",
    "- Introduced anchor boxes to improve localization of multiple objects.\n",
    "- Trained jointly on ImageNet and COCO, allowing it to detect over 9000 object categories.\n",
    "- Marked a significant step towards more scalable detection.\n",
    "\n",
    "### YOLOv3 (2018)\n",
    "- Further improved with the Darknet-53 backbone (deeper and more powerful).\n",
    "- Supports multi-scale detection (feature maps at three different scales).\n",
    "- Uses logistic classifiers and independent object class predictions.\n",
    "- Balanced well between speed and accuracy; became one of the most widely adopted versions.\n",
    "\n",
    "### YOLOv4 (2020)\n",
    "- Developed by Alexey Bochkovskiy.\n",
    "- Combined multiple training tricks and improvements (e.g., Mosaic augmentation, Mish activation).\n",
    "- Optimized for both GPU and CPU performance.\n",
    "- Open-source and compatible with OpenCV and TensorRT.\n",
    "\n",
    "### YOLOv5 (2020)\n",
    "- Released by Ultralytics (not by the original authors).\n",
    "- Implemented in PyTorch, making it more accessible to the community.\n",
    "- Provided pre-trained models in multiple sizes (s, m, l, x).\n",
    "- Included tools for training, inference, and export to various deployment formats.\n",
    "\n",
    "### YOLOv6 and YOLOv7 (2022)\n",
    "- YOLOv6 (by Meituan): focused on industrial applications, written in PyTorch.\n",
    "- YOLOv7 (by WongKinYiu): pushed the limit of real-time object detection with high accuracy.\n",
    "- Introduced features like E-ELAN blocks and model reparameterization.\n",
    "\n",
    "### YOLOv8 (2023)\n",
    "- Major rewrite by Ultralytics with a new architecture, no longer based on previous YOLO codebases.\n",
    "- Supports multiple vision tasks: detection, instance segmentation, classification, pose estimation.\n",
    "- Unified interface and modern design, exported easily to ONNX, TensorRT, and other formats.\n",
    "\n",
    "### YOLOv9, v10, v11 (2024)\n",
    "- YOLOv9: Introduced GELAN architecture and PGI module for improved accuracy and efficiency.\n",
    "- YOLOv10: Focused on end-to-end detection without post-processing (NMS-free).\n",
    "- YOLOv11: Further optimized the architecture for low-latency applications; introduced new CSP-based components.\n",
    "\n",
    "### YOLOv12 (2025)\n",
    "- Latest known version as of February 2025.\n",
    "- Documentation still limited, but represents the continuation of the Ultralytics line with improvements in performance, scalability, and support for additional tasks.\n",
    "\n",
    "\n",
    "\n",
    "To begin and carry out initial experiments, version 5 is recommended, as it offers a good balance between usability (low complexity) and the quality of the results that can be achieved. Another good entry point for using YOLO is version 8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19f761-f55c-4010-b799-930925b8e90f",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "The network consists of 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by [GoogLeNet](https://arxiv.org/pdf/1409.4842), the network reduces activation maps using 1×1 convolutional layers followed by 3×3 convolutional layers.\n",
    "\n",
    "![YOLO](../assets/bloc2/YOLO.png \"YOLO\")\n",
    "\n",
    "### Unified Detection\n",
    "\n",
    "The paper explains:\n",
    "\n",
    "> We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes for all classes in an image simultaneously. This means that our network reasons globally about the full image and all objects within it.\n",
    "\n",
    "The YOLO system divides the input image into an $S×S$ grid. If the center of an object falls within a grid cell, that cell is responsible for detecting the object. Each grid cell predicts $B$ bounding boxes and confidence scores for those boxes.\n",
    "\n",
    "These confidence scores reflect the model's certainty that a box contains an object, as well as how accurate it believes the predicted box is. Each bounding box consists of five predictions: $x, y, w, h$ and confidence. The coordinates $(x, y)$ represent the center of the box relative to the grid cell boundaries. The width and height are predicted relative to the entire image. Finally, the confidence score represents the Intersection over Union (IoU) between the predicted box and any ground truth box. Each grid cell also predicts $C$ conditional class probabilities.\n",
    "\n",
    "![YOLO](../assets/bloc2/YOLO_deteccio.png \"YOLO\")\n",
    "\n",
    "The architecture described above has an output layer of size $7×7×30$, resulting from the formula: $S × S × (B × 5 + C)$.  \n",
    "In the original paper: $S = 7$, $B = 2$, and $C = 20$, since the model was trained on the [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset.\n",
    "\n",
    "Modern YOLO architectures are significantly more complex and are typically divided into three main components:\n",
    "\n",
    "- **Backbone**: A convolutional network that extracts features from the image. From version 3 onwards, YOLO uses its own architecture called *DarkNet*, a residual network with approximately 53 layers.\n",
    "- **Neck**: This component connects the backbone to the head(s). It is responsible for tasks such as multi-scale object detection, using feature pyramid networks that aggregate information from different stages of the backbone.\n",
    "- **Head**: The head performs the final predictions. In modern versions of YOLO, multiple detection modules are used to predict bounding boxes, objectness scores, and class probabilities for each grid cell in the feature map. These predictions are then aggregated to produce the final detections.\n",
    "\n",
    "An example of this more complex architecture can be found in the official documentation for [YOLO v5](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/#1-model-structure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3b32c-07ee-4aa3-918d-99807c171dae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using YOLO\n",
    "\n",
    "The simplest way to use the network is through the library provided by the company Ultralytics. This makes it very easy to experiment with different versions of the network, as well as to perform fine-tuning and transfer learning processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c442c9-a9a9-475f-84d4-b44d88c2480b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6337fa5-33c5-4ed8-92d2-248032570989",
   "metadata": {},
   "source": [
    "We will start by experimenting with YOLOv5, which comes in five different versions. Each version features a backbone network of a different size. In addition, there are two available input image sizes.\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>size<br><sup>(pixels)</sup></th>\n",
    "      <th>mAP<sup>val<br>50-95</sup></th>\n",
    "      <th>mAP<sup>val<br>50</sup></th>\n",
    "      <th>Speed<br><sup>CPU b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b32<br>(ms)</sup></th>\n",
    "      <th>params<br><sup>(M)</sup></th>\n",
    "      <th>FLOPs<br><sup>@640 (B)</sup></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt\" target=\"_blank\">YOLOv5n</a></td>\n",
    "      <td>640</td>\n",
    "      <td>28.0</td>\n",
    "      <td>45.7</td>\n",
    "      <td><strong>45</strong></td>\n",
    "      <td><strong>6.3</strong></td>\n",
    "      <td><strong>0.6</strong></td>\n",
    "      <td><strong>1.9</strong></td>\n",
    "      <td><strong>4.5</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\" target=\"_blank\">YOLOv5s</a></td>\n",
    "      <td>640</td>\n",
    "      <td>37.4</td>\n",
    "      <td>56.8</td>\n",
    "      <td>98</td>\n",
    "      <td>6.4</td>\n",
    "      <td>0.9</td>\n",
    "      <td>7.2</td>\n",
    "      <td>16.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt\" target=\"_blank\">YOLOv5m</a></td>\n",
    "      <td>640</td>\n",
    "      <td>45.4</td>\n",
    "      <td>64.1</td>\n",
    "      <td>224</td>\n",
    "      <td>8.2</td>\n",
    "      <td>1.7</td>\n",
    "      <td>21.2</td>\n",
    "      <td>49.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt\" target=\"_blank\">YOLOv5l</a></td>\n",
    "      <td>640</td>\n",
    "      <td>49.0</td>\n",
    "      <td>67.3</td>\n",
    "      <td>430</td>\n",
    "      <td>10.1</td>\n",
    "      <td>2.7</td>\n",
    "      <td>46.5</td>\n",
    "      <td>109.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt\" target=\"_blank\">YOLOv5x</a></td>\n",
    "      <td>640</td>\n",
    "      <td>50.7</td>\n",
    "      <td>68.9</td>\n",
    "      <td>766</td>\n",
    "      <td>12.1</td>\n",
    "      <td>4.8</td>\n",
    "      <td>86.7</td>\n",
    "      <td>205.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt\" target=\"_blank\">YOLOv5n6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>36.0</td>\n",
    "      <td>54.4</td>\n",
    "      <td>153</td>\n",
    "      <td>8.1</td>\n",
    "      <td>2.1</td>\n",
    "      <td>3.2</td>\n",
    "      <td>4.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt\" target=\"_blank\">YOLOv5s6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>44.8</td>\n",
    "      <td>63.7</td>\n",
    "      <td>385</td>\n",
    "      <td>8.2</td>\n",
    "      <td>3.6</td>\n",
    "      <td>12.6</td>\n",
    "      <td>16.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt\" target=\"_blank\">YOLOv5m6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>51.3</td>\n",
    "      <td>69.3</td>\n",
    "      <td>887</td>\n",
    "      <td>11.1</td>\n",
    "      <td>6.8</td>\n",
    "      <td>35.7</td>\n",
    "      <td>50.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt\" target=\"_blank\">YOLOv5l6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>53.7</td>\n",
    "      <td>71.3</td>\n",
    "      <td>1784</td>\n",
    "      <td>15.8</td>\n",
    "      <td>10.5</td>\n",
    "      <td>76.8</td>\n",
    "      <td>111.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt\" target=\"_blank\">YOLOv5x6</a><br>+ [TTA]</td>\n",
    "      <td>1280<br>1536</td>\n",
    "      <td>55.0<br><strong>55.8</strong></td>\n",
    "      <td>72.7<br><strong>72.7</strong></td>\n",
    "      <td>3136<br>-</td>\n",
    "      <td>26.2<br>-</td>\n",
    "      <td>19.4<br>-</td>\n",
    "      <td>140.7<br>-</td>\n",
    "      <td>209.8<br>-</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "We will begin by testing the smallest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f0f76f9-1c2c-496c-97b8-b5bbebff4ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T07:04:42.442994Z",
     "start_time": "2024-11-25T07:04:33.126513Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5n.pt' with new 'model=yolov5nu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "YOLOv5n summary: 262 layers, 2,654,816 parameters, 0 gradients, 7.8 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(262, 2654816, 0, 7.840102399999999)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv5n model\n",
    "model = YOLO(\"yolov5n.pt\") #yolov5nu.pt\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980eac5e-e39f-4ebe-8389-d6869976fbfc",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "YOLOv5 has been trained on the COCO (Common Objects in Context) dataset [link](https://cocodataset.org/#home), which contains 80 different classes. Performing inference for detection is very simple; it is enough to call the model. The model returns an object of type _Results_. \n",
    "\n",
    "[Documentation](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Results).\n",
    "\n",
    "Thus, the inference process uses the Ultralytics API and abstracts away from PyTorch.\n",
    "\n",
    "- YOLO accepts various input formats, including:\n",
    "  - **File path**: Path to an image or video file (e.g., `\"image.jpg\"`).\n",
    "  - **List of file paths**: Multiple images for batch processing (e.g., `[\"img1.jpg\", \"img2.jpg\"]`).\n",
    "  - **NumPy array**: An image loaded as a NumPy array (make sure it’s RGB).\n",
    "  - **PyTorch tensor**: A tensor of shape `[C, H, W]` or `[B, C, H, W]` with pixel values.\n",
    "  - **Directory path**: Folder containing images.\n",
    "    \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac12e4c3-36ba-4812-af92-7c543bba6710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T20:38:17.665132Z",
     "start_time": "2025-06-25T20:38:17.657151Z"
    }
   },
   "outputs": [],
   "source": [
    "img = \"https://hips.hearstapps.com/hmg-prod/images/the-boys-serie-amazon-1565605836.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16543479-afdc-41e8-888c-197a5233e2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://hips.hearstapps.com/hmg-prod/images/the-boys-serie-amazon-1565605836.jpg locally at the-boys-serie-amazon-1565605836.jpg\n",
      "image 1/1 C:\\Users\\gabri\\PycharmProjects\\aa_2425\\14_YOLO\\the-boys-serie-amazon-1565605836.jpg: 448x640 5 persons, 2 handbags, 2 ties, 161.6ms\n",
      "Speed: 0.0ms preprocess, 161.6ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "results = model(img)\n",
    "results[0]; # It's a results list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8dfad0-85c1-4b9e-b9ec-6295b3fe89a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([ 0.,  0.,  0.,  0., 26.,  0., 26., 27., 27.])\n",
       "conf: tensor([0.9311, 0.8853, 0.8708, 0.8288, 0.3462, 0.3201, 0.2963, 0.2759, 0.2705])\n",
       "data: tensor([[1.1957e+03, 4.9963e+01, 1.9991e+03, 1.3012e+03, 9.3113e-01, 0.0000e+00],\n",
       "        [5.1376e+02, 1.6846e+02, 1.1217e+03, 1.3078e+03, 8.8529e-01, 0.0000e+00],\n",
       "        [1.1506e+00, 3.3177e+02, 3.4585e+02, 1.2959e+03, 8.7082e-01, 0.0000e+00],\n",
       "        [1.8088e+02, 3.4668e+02, 5.2710e+02, 1.3006e+03, 8.2880e-01, 0.0000e+00],\n",
       "        [3.6379e+02, 1.0700e+03, 5.5296e+02, 1.3080e+03, 3.4624e-01, 2.6000e+01],\n",
       "        [1.0214e+03, 4.3079e+02, 1.1316e+03, 1.0513e+03, 3.2006e-01, 0.0000e+00],\n",
       "        [4.2099e+02, 1.0726e+03, 5.5326e+02, 1.3072e+03, 2.9633e-01, 2.6000e+01],\n",
       "        [3.4559e+02, 5.9484e+02, 4.5085e+02, 8.1905e+02, 2.7592e-01, 2.7000e+01],\n",
       "        [3.4424e+02, 5.9315e+02, 4.3852e+02, 7.6313e+02, 2.7049e-01, 2.7000e+01]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (1310, 2000)\n",
       "shape: torch.Size([9, 6])\n",
       "xywh: tensor([[1597.4166,  675.5958,  803.3340, 1251.2653],\n",
       "        [ 817.7532,  738.1421,  607.9879, 1139.3711],\n",
       "        [ 173.5022,  813.8619,  344.7030,  964.1741],\n",
       "        [ 353.9908,  823.6443,  346.2124,  953.9302],\n",
       "        [ 458.3753, 1189.0438,  189.1771,  238.0100],\n",
       "        [1076.4984,  741.0664,  110.1166,  620.5527],\n",
       "        [ 487.1258, 1189.8694,  132.2773,  234.6008],\n",
       "        [ 398.2217,  706.9449,  105.2539,  224.2147],\n",
       "        [ 391.3820,  678.1398,   94.2835,  169.9805]])\n",
       "xywhn: tensor([[0.7987, 0.5157, 0.4017, 0.9552],\n",
       "        [0.4089, 0.5635, 0.3040, 0.8697],\n",
       "        [0.0868, 0.6213, 0.1724, 0.7360],\n",
       "        [0.1770, 0.6287, 0.1731, 0.7282],\n",
       "        [0.2292, 0.9077, 0.0946, 0.1817],\n",
       "        [0.5382, 0.5657, 0.0551, 0.4737],\n",
       "        [0.2436, 0.9083, 0.0661, 0.1791],\n",
       "        [0.1991, 0.5397, 0.0526, 0.1712],\n",
       "        [0.1957, 0.5177, 0.0471, 0.1298]])\n",
       "xyxy: tensor([[1.1957e+03, 4.9963e+01, 1.9991e+03, 1.3012e+03],\n",
       "        [5.1376e+02, 1.6846e+02, 1.1217e+03, 1.3078e+03],\n",
       "        [1.1506e+00, 3.3177e+02, 3.4585e+02, 1.2959e+03],\n",
       "        [1.8088e+02, 3.4668e+02, 5.2710e+02, 1.3006e+03],\n",
       "        [3.6379e+02, 1.0700e+03, 5.5296e+02, 1.3080e+03],\n",
       "        [1.0214e+03, 4.3079e+02, 1.1316e+03, 1.0513e+03],\n",
       "        [4.2099e+02, 1.0726e+03, 5.5326e+02, 1.3072e+03],\n",
       "        [3.4559e+02, 5.9484e+02, 4.5085e+02, 8.1905e+02],\n",
       "        [3.4424e+02, 5.9315e+02, 4.3852e+02, 7.6313e+02]])\n",
       "xyxyn: tensor([[5.9787e-01, 3.8140e-02, 9.9954e-01, 9.9330e-01],\n",
       "        [2.5688e-01, 1.2859e-01, 5.6087e-01, 9.9834e-01],\n",
       "        [5.7532e-04, 2.5326e-01, 1.7293e-01, 9.8927e-01],\n",
       "        [9.0442e-02, 2.6464e-01, 2.6355e-01, 9.9283e-01],\n",
       "        [1.8189e-01, 8.1682e-01, 2.7648e-01, 9.9851e-01],\n",
       "        [5.1072e-01, 3.2885e-01, 5.6578e-01, 8.0255e-01],\n",
       "        [2.1049e-01, 8.1875e-01, 2.7663e-01, 9.9784e-01],\n",
       "        [1.7280e-01, 4.5407e-01, 2.2542e-01, 6.2523e-01],\n",
       "        [1.7212e-01, 4.5279e-01, 2.1926e-01, 5.8254e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "r = results[0]\n",
    "\n",
    "r.boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f32c8-bce1-4cf8-8dc6-f50d33ad3421",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To perform training, the `train` method of the `YOLO` class is used. There is no need to implement a training loop manually, as this function provides a higher-level abstraction. It is important to note that this method is highly configurable, so it is recommended to study its parameters carefully before starting a training session.\n",
    "\n",
    "Refer to the documentation [link](https://docs.ultralytics.com/modes/train/#key-features-of-train-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8d7e8-92c5-4286-89ae-3dad9c53a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the official documentation we take som examples\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO model\n",
    "model = YOLO(\"yolov5n.pt\") # NOTE: It is also possible to load a model without pretraining. These models are defined in `.yaml` files.\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640) # NOTE: We can train here because `coco8` is included within `ultralytics`.\n",
    "\n",
    "# Run inference\n",
    "results = model(\"path/to/imatge.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c03c97-50b1-4ddf-8773-ad5106f91503",
   "metadata": {},
   "source": [
    "### Segmentation using YOLO\n",
    "\n",
    "It is important to note that YOLO (You Only Look Once) was originally developed for object detection tasks, not for segmentation. Its architecture was initially optimized for real-time detection performance, focusing on identifying and localizing objects within bounding boxes. The integration of segmentation capabilities is a more recent extension, made possible through architectural modifications in later versions such as YOLOv5 (with custom tweaks) and more naturally in YOLOv8 with dedicated segmentation heads.\n",
    "\n",
    "\n",
    "Although segmentation tasks can be performed by modifying version 5 (see [link](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb)), it is from version 8 onward that this task is natively integrated into the network through the addition of a new head specifically for segmentation.\n",
    "\n",
    "In the documentation, we can see that there are now versions of all the weight files available for the different tasks: [link](https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes).\n",
    "\n",
    "Below, we will look at an example of segmentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3c55c-49d1-497c-90e4-0351f9500bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "results_seg = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2e79a-b4c6-4fdf-ac1a-dda8be38f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_result = results_seg[0].plot()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img_result);\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
