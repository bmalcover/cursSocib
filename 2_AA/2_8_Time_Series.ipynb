{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/cursSocib/blob/main/2_AA/2_8_Time_Series.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Deep Learning for Time Series\n",
    "\n",
    "\n",
    "Traditional time series models are effective for small-scale, linear, and stationary data. However, real-world time series are often:\n",
    "\n",
    "- **Non-linear** (e.g., financial markets, weather patterns)\n",
    "- **High-dimensional** (e.g., multiple sensors, multivariate financial indicators)\n",
    "- **Non-stationary** (e.g., concept drift, changing seasonality)\n",
    "- **Noisy and incomplete** (e.g., IoT sensor dropouts)\n",
    "\n",
    "Deep learning offers several key advantages:\n",
    "\n",
    "- **Learns complex patterns**: Can model non-linear relationships and long-range dependencies.\n",
    "- **Scales to large data**: Handles high-volume, high-frequency data without manual feature engineering.\n",
    "- **Supports multivariate inputs**: Learns interactions between multiple time series.\n",
    "- **Flexible input/output formats**: Sequence-to-sequence architectures allow custom horizons and multistep forecasting.\n",
    "- **Robust to noise**: Neural networks can filter and learn over noisy, redundant inputs.\n",
    "\n",
    "As a result, deep learning has become essential in domains where:\n",
    "- Feature interactions are unknown or too complex to hand-craft,\n",
    "- Patterns evolve over time (non-stationarity),\n",
    "- Large, heterogeneous datasets are available.\n"
   ],
   "id": "2ea1fae3cd96c0ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ],
   "id": "9ebc1958c0823d84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Daily SST, Gulf of Mexico\n",
    "\n",
    "The data is obtained from this [Link](https://climatereanalyzer.org/clim/sst_daily/?dm_id=gomex). Our goal is to predict is to forecast the temperature.\n",
    "\n",
    "> This page provides time series and map visualizations of daily mean Sea Surface Temperature (SST) from NOAA Optimum Interpolation SST (OISST) version 2.1. OISST is a 0.25°x0.25° gridded dataset that provides estimates of temperature based on a blend of satellite, ship, and buoy observations. The datset spans 1 September 1981 to present with a 1 to 2-day lag from the current day. Data are preliminary for about two weeks until a finalized product is posted by NOAA. This status is identified on the maps by \"preliminary\" appearing in the title, and applies to the time series as well. Learn more about OISST, including strengths and limitations, from the NCAR Climate Data Guide.\n",
    "\n",
    "\n",
    "<img src=\"../assets/bloc2/data.png\" />"
   ],
   "id": "314493e913c73c1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!curl https://climatereanalyzer.org/clim/sst_daily/json_2clim/oisst2.1_gomex_sst_day.json -o mexico_gulf.json",
   "id": "4403e405dd2cbf75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('mexico_gulf.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "info = list()\n",
    "for year_info in data[1:-3]:\n",
    "    print(year_info['name'])\n",
    "    info += year_info[\"data\"]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(info)"
   ],
   "id": "e4ba5f9fed1dffb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data cleaning\n",
    "\n",
    "We remove the first and the last year due to the presence of NaNs"
   ],
   "id": "3f8e4a8184f0dca6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train = df[:-365]\n",
    "x_val = df[-365:]"
   ],
   "id": "567aa5a1cf9f7dfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Time series data often comes from sensors, logs, APIs, or manual inputs — and it's rarely clean out of the box. **Data cleaning** is a crucial first step before any time series analysis or modeling. Cleaning ensures that patterns in the data reflect reality, not noise or errors.\n",
    "\n",
    "| Method                          | Description                                  | Syntax Example                           |\n",
    "|--------------------------------|----------------------------------------------|-------------------------------------------|\n",
    "| Drop NaNs                      | Removes rows with NaNs                       | `df.dropna()`                             |\n",
    "| Forward Fill                   | Fills NaN with previous value                | `df.ffill()`                              |\n",
    "| Backward Fill                  | Fills NaN with next value                    | `df.bfill()`                              |\n",
    "| Fill with Fixed Value          | Replaces NaN with a specified value          | `df.fillna(0)`                            |\n",
    "| Fill with Mean                 | Replaces NaN with column mean                | `df.fillna(df.mean())`                    |\n",
    "| Fill with Median               | Replaces NaN with column median              | `df.fillna(df.median())`                  |\n",
    "| Interpolate                    | Linearly interpolate missing values          | `df.interpolate()`                        |\n",
    "| Fill Within Group (ffill)      | Forward fill within group (e.g., by date)    | `df.groupby('group_col').ffill()`         |\n",
    "| Fill Within Group (mean)       | Fill with mean within each group             | `df.groupby('group_col').transform('mean')` |\n",
    "| Rolling Fill                   | Use rolling stats (e.g., mean) to fill       | `df.fillna(df.rolling(3).mean())`         |\n"
   ],
   "id": "bad27aa77ee2602d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_train.isnull().sum()",
   "id": "da85dbb731c89a7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will use ``ffill`` method to remove the NaNs.",
   "id": "6920f6278261d92b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train = x_train.ffill()\n",
    "x_train.isnull().sum()"
   ],
   "id": "cab62b22431b7cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train = torch.from_numpy(x_train.values)\n",
    "x_val = torch.from_numpy(x_val.values)"
   ],
   "id": "538f58b82b414f9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_batches(data, window):\n",
    "    \"\"\"\n",
    "    Takes data with shape (n_samples, n_features) and creates mini-batches\n",
    "    with shape (1, window).\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(data)\n",
    "    for i in range(L - (window + 1)):\n",
    "        x_sequence = data[i:i + window]\n",
    "        y_sequence = data[i + window + 1]\n",
    "\n",
    "        yield x_sequence, y_sequence"
   ],
   "id": "f65eb1ce127a68d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model: RNN\n",
    "\n",
    "![](https://www.mdpi.com/information/information-15-00517/article_deploy/html/images/information-15-00517-g001-550.jpg)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed for **sequence data**. Unlike traditional feedforward neural networks, RNNs have connections that loop backward, allowing them to maintain a **memory of previous inputs**. This makes them ideal for tasks like:\n",
    "\n",
    "- Time series forecasting\n",
    "- Natural language processing\n",
    "- Speech recognition\n",
    "- Sequential data classification\n",
    "\n",
    "## How RNNs Work\n",
    "\n",
    "At each time step $t$, an RNN takes an input $x_t$ and the previous hidden state $h_{t-1}$, and computes the new hidden state $h_t$:\n",
    "\n",
    "$$\n",
    "    h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_t$: input at time $t$\n",
    "- $h_t$: hidden state at time $t$\n",
    "- $W_{xh}$, $W_{hh}$: weight matrices\n",
    "- $b_h$: bias term\n",
    "\n",
    "The hidden state $h_t$ acts as a summary of everything the network has seen up to time $t$.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Hidden state ($h_t$)**: Carries information from previous time steps.\n",
    "- **Weights are shared** across all time steps, making RNNs efficient for sequences.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- RNNs struggle with **long-term dependencies** due to vanishing gradients.\n",
    "- Variants like **LSTM** and **GRU** are designed to solve this problem.\n",
    "\n",
    "RNNs are a foundational building block in deep learning for sequential tasks, and understanding them is key to mastering time-based data models."
   ],
   "id": "89aaa47ce712291b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features = 1  # Features\n",
    "n_hidden = 200  # Nodes\n",
    "n_layers = 2  # Layers\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(features, n_hidden, n_layers)\n",
    "        self.fc = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "id": "9b1e1673da349465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "RNNs output follows always the shape:\n",
    "\n",
    "``batch_size, sequence_length, hidden_units``\n",
    "\n",
    "Remember that for the recurrent nature of these networks the last information is in the last element of the sequence. To make predictions we use this information. The hidden units are similar to the neurons for FCN and number of filters for CNNs."
   ],
   "id": "944fe50ff889146c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "net = RNN()\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for _ in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    i = 0\n",
    "    t_loss = 0\n",
    "    for i, (x, y) in enumerate(get_batches(x_train, 12)):\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Create batch_size dimension\n",
    "        x = x.unsqueeze(0)\n",
    "        out = net(x.float())\n",
    "\n",
    "        loss = criterion(out, y.reshape(1,1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "\n",
    "    train_loss.append(t_loss / i + 1)\n",
    "\n",
    "    val_loss = 0\n",
    "    i = 0\n",
    "    for i, (x, y) in enumerate(get_batches(x_val, 12)):\n",
    "        net.eval()\n",
    "\n",
    "        x = x.unsqueeze(0)\n",
    "        out = net(x.float())\n",
    "        loss = criterion(out, y.reshape(1,1).float())\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    valid_loss.append(val_loss / i + 1)"
   ],
   "id": "7f762230093c3a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.title(\"Train loss\")\n",
    "plt.plot(train_loss);"
   ],
   "id": "df6deeba7259a4c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model: LSTM\n",
    "\n",
    "A good explanation: [Link](https://la.mathworks.com/discovery/lstm.html)\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** networks are a specialized type of **Recurrent Neural Network (RNN)** designed to model sequential data and capture long-term dependencies. Unlike traditional RNNs, which suffer from the **vanishing gradient problem**, LSTMs can learn to retain or forget information over long sequences using a more complex internal structure.\n",
    "\n",
    "Standard RNNs update their hidden state $h_t$ at each time step based on the current input $x_t$ and the previous hidden state $h_{t-1}$. However, as the sequence length increases, the network struggles to retain relevant past information due to vanishing or exploding gradients during training.\n",
    "\n",
    "LSTMs address this by introducing a **memory cell** that can preserve information across many time steps, controlled by gates that regulate the flow of information.\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "![](https://la.mathworks.com/discovery/lstm/_jcr_content/mainParsys/band/mainParsys/lockedsubnav/mainParsys/columns_1332042868/09887e7d-81a1-4a53-b298-eb7bd9d6ac8c/image.adapt.full.medium.jpg/1747387158162.jpg)\n",
    "\n",
    "Each LSTM cell has three primary gates:\n",
    "\n",
    "- **Forget Gate $f_t$**: Decides what information to discard from the previous cell state.\n",
    "- **Input Gate $i_t$**: Determines which new values to add to the cell state.\n",
    "- **Output Gate $o_t$**: Controls the information passed to the next hidden state.\n",
    "\n",
    "Additionally:\n",
    "\n",
    "- **Cell State $C_t$**: Acts as a memory conveyor belt, updated at each step using the input and forget gates.\n",
    "- **Hidden State $h_t$**: The output of the LSTM cell at each time step.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "The update equations for an LSTM cell at time step $t$ are:\n",
    "\n",
    "\n",
    "**Equations of LSTM**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) & \\text{(Forget gate)} \\\\\n",
    "i_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) & \\text{(Input gate)} \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_c x_t + U_c h_{t-1} + b_c) & \\text{(Candidate cell state)} \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t & \\text{(Updated cell state)} \\\\\n",
    "o_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) & \\text{(Output gate)} \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t) & \\text{(Updated hidden state)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "| Symbol          | Meaning                          |\n",
    "| --------------- | -------------------------------- |\n",
    "| $x_t$           | Input vector at time $t$         |\n",
    "| $h_t$           | Hidden state at time $t$         |\n",
    "| $C_t$           | Cell state (memory) at time $t$  |\n",
    "| $f_t, i_t, o_t$ | Forget, input, and output gates  |\n",
    "| $\\tilde{C}_t$   | Candidate values for cell state  |\n",
    "| $W_*, U_*, b_*$ | Weights and biases for each gate |\n",
    "| $\\sigma$        | Sigmoid activation function      |\n",
    "| $\\tanh$         | Hyperbolic tangent activation    |\n",
    "| $\\odot$         | Element-wise multiplication      |\n",
    "\n",
    "\n",
    "#### RNN vs LSTM\n",
    "\n",
    "| Feature                           | RNN                                                                  | LSTM                                                              |\n",
    "| --------------------------------- | -------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| **Memory capability**             | Short-term memory only; struggles with long sequences.               | Designed for **long-term memory** using a memory cell.            |\n",
    "| **Structure**                     | Simple loop with one hidden state update.                            | Complex cell with **three gates**: input, forget, and output.     |\n",
    "| **Vanishing gradient problem**    | Common issue during training with long sequences.                    | Much **less prone** to vanishing gradients.                       |\n",
    "| **Gates**                         | ❌ No gates. Just applies an activation function to the hidden state. | ✅ Uses gates to control **what to remember, forget, and output**. |\n",
    "| **Training time**                 | Faster to train (fewer parameters).                                  | Slower to train (more parameters, complex structure).             |\n",
    "| **Performance on long sequences** | Poor, tends to forget early inputs.                                  | Excellent at **capturing long-term dependencies**.                |\n",
    "| **Use cases**                     | Simple or short sequences.                                           | Complex, long sequences like text, audio, time series.            |\n",
    "\n",
    "\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "- Use RNN when:\n",
    "    - The sequence is short.\n",
    "    - You need something lightweight.\n",
    "    - You’re just experimenting or prototyping.\n",
    "\n",
    "\n",
    "\n",
    "- Use LSTM when:\n",
    "    - The sequence is long or has dependencies far apart.\n",
    "    - You need accurate memory retention (e.g., text generation, time series prediction).\n",
    "    - Vanishing gradients are a problem."
   ],
   "id": "c04d749f246c737a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_size = 1 # Features\n",
    "n_hidden = 200 # Nodes\n",
    "n_layers = 2 # Layers\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, n_hidden, n_layers)\n",
    "        self.fc = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "id": "a7a65f62d48b2bc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "net = LSTM()\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for e in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    t_loss = 0\n",
    "    for i, (x, y) in enumerate(get_batches(x_train, 12)):\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Create batch_size dimension\n",
    "        x = x.unsqueeze(0)\n",
    "        out = net(x.float())\n",
    "\n",
    "        loss = criterion(out, y.reshape(1,1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "\n",
    "    train_loss.append(t_loss / i)"
   ],
   "id": "6857d5952ffac94d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b8e4c641f7fa179"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
