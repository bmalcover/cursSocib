{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/cursSocib/blob/main/2_AA/2_2_Images.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "# Deep Learning for images\n",
    "\n",
    "In previous lessons, we've worked extensively with **tabular data**—structured datasets where each row represents an observation and each column represents a feature. These datasets are typically easy to represent and analyze using tools like `pandas`, `scikit-learn`, and various statistical models.\n",
    "\n",
    "However, **images** are fundamentally different in both structure and content. Instead of rows and columns of explicitly labeled features, an image is a grid of pixel values—typically in 2D for grayscale or 3D for color (height × width × channels). For example, a color image of size 224×224 pixels with 3 color channels (RGB) contains over 150,000 raw values, none of which are labeled with human-interpretable features like \"age\" or \"income\".\n",
    "\n",
    "![DL 4 Img](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/3_what-is-deep-learning.png)\n",
    "\n",
    "\n",
    "This difference leads to several important challenges:\n",
    "\n",
    "- **High Dimensionality**: Images contain many more features (pixels) than typical tabular datasets, which increases computational complexity and the risk of overfitting.\n",
    "- **Spatial Structure**: Nearby pixels in an image are often related, forming edges, textures, and patterns. Tabular models generally do not capture such local dependencies.\n",
    "- **Semantic Gap**: The relationship between raw pixels and meaningful concepts (like a cat, a face, or a traffic sign) is complex and non-linear, requiring sophisticated models to bridge this gap.\n",
    "\n",
    "Because of these challenges, we need **image-specific models**, most notably **Convolutional Neural Networks (CNNs)**, which are designed to exploit the spatial and hierarchical structure of images. These models learn to recognize patterns such as edges, shapes, and eventually objects, by processing images in layers of increasing abstraction.\n",
    "\n",
    "In this module, we will explore how to work with image data, and why specialized techniques are crucial for tasks such as image classification, object detection, and image generation. To do so we will use again `PyTorch`.\n",
    "\n",
    "## Batches\n",
    "\n",
    "When training deep learning models with images, we don't feed the whole dataset, as we dit for tabular data. Instead, we group multiple images into **batches**.\n",
    "\n",
    "A **batch** is a collection of samples (e.g., images and their labels) processed together in a single forward and backward pass.\n",
    "\n",
    "### Why Batching?\n",
    "\n",
    "- **Efficiency**: Batches make better use of GPU parallelism, speeding up training.\n",
    "- **Memory Management**: Handling smaller groups instead of the full dataset helps manage limited GPU memory.\n",
    "- **Gradient Stability**: Averaging gradients over a batch reduces noise in weight updates.\n",
    "\n",
    "### Terminology\n",
    "\n",
    "- **Batch Size**: The number of samples in a single batch (e.g., 32, 64, or 128). This is a key hyperparameter.\n",
    "- **Epoch**: One complete pass through the entire dataset.\n",
    "- **Iteration**: One update step; corresponds to processing one batch.\n",
    "\n",
    "## The MNIST Dataset\n",
    "\n",
    "To begin working with image data in practice, we'll use one of the most well-known benchmark datasets: **MNIST**.\n",
    "\n",
    "**MNIST** stands for *Modified National Institute of Standards and Technology* database. It consists of **70,000 grayscale images** of handwritten digits (0 through 9), split into:\n",
    "\n",
    "- **60,000 training images**\n",
    "- **10,000 test images**\n",
    "\n",
    "Each image is:\n",
    "\n",
    "- **28 × 28 pixels**\n",
    "- **Grayscale** (i.e., a single channel)\n",
    "- **Labeled** with the correct digit (0–9)\n",
    "\n",
    "This dataset is ideal for learning because it's small, clean, and already preprocessed, while still offering real-world complexity in handwritten digits.\n",
    "\n",
    "### Example MNIST Images\n",
    "\n",
    "Below is a sample of MNIST digits:\n",
    "\n",
    "![MNIST Examples](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "Each row in the image above shows digits 0–9 written by different people. As you can see, some digits are written in very different ways, which is why we need machine learning to recognize them automatically.\n",
    "\n",
    "We simplify the problem, making it a binary problem: **is a 5 or not?**"
   ],
   "id": "4415ef93c445cad1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from IPython.core.pylabtools import figsize\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm"
   ],
   "id": "f86bb5a555819aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_train = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Loop through batches\n",
    "for images, labels in dataloader_train:\n",
    "    print(images.shape)  # torch.Size([64, 3, 32, 32])\n",
    "    break"
   ],
   "id": "6464983fe0e8d412"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`nn.Sequential` is a simple way to build a neural network by **stacking layers in order**.\n",
    "\n",
    "It’s useful when your model is a straight chain of layers, with no branching or custom logic."
   ],
   "id": "1a9125037f4d3976"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mlp_net = nn.Sequential(\n",
    "    torch.nn.Linear(784, 10),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(10, 1)\n",
    ")"
   ],
   "id": "df740d165d9540e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because the task to solve is a binary classification problem, we use, once again, the `BCEWithLogitsLoss()` loss function.",
   "id": "3ad8618342209186"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "EPOCHS = 5\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(mlp_net.parameters(), lr=LR)"
   ],
   "id": "6c139462424099c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "running_loss = []\n",
    "running_acc = []\n",
    "\n",
    "running_test_loss = []\n",
    "running_test_acc = []\n",
    "\n",
    "for t in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    i_batch = 0\n",
    "    for i_batch, (x, y) in enumerate(dataloader_train):  # We have to iter the batches.\n",
    "        mlp_net.train()\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten images\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mlp_net(x)\n",
    "\n",
    "        # 1. LOSS CALCULATION\n",
    "        y_binary = (y == 5)  # Simplification\n",
    "        y_binary = y_binary.double()\n",
    "        y_binary = y_binary.reshape(-1, 1)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_binary)\n",
    "\n",
    "        # 2. GRADIENT\n",
    "        mlp_net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # 3. OPTIMISATION\n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "        # 4. EVALUATION\n",
    "        mlp_net.eval()  # Mode avaluació de la xarxa\n",
    "\n",
    "        y_pred = mlp_net(x)\n",
    "        y_pred_binary = (y_pred > 0.5).double()\n",
    "\n",
    "        batch_loss += (loss_fn(y_pred, y_binary).detach())\n",
    "        batch_acc += accuracy_score(y_pred_binary.detach(), y_binary.detach())\n",
    "\n",
    "    running_loss.append(batch_loss / (i_batch + 1))\n",
    "    running_acc.append(batch_acc / (i_batch + 1))\n"
   ],
   "id": "2c4528ef1e8ed200"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(running_acc);"
   ],
   "id": "c69c0eb716ae143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CNNs\n",
    "\n",
    "Until now we have used MLP. In the previous example wa had to flatten the data. Therefore, happens the following:"
   ],
   "id": "b1abff328d518772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images[0, 0], cmap=\"Greys\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.hstack(([images[0, 0].flatten()] * 50)).reshape(50, -1), cmap=\"Greys\");  # To improve the width\n",
    "plt.yticks([]);"
   ],
   "id": "796b9a5511d618a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see it we make a complex problem even worse. For `MNIST` there is not problem. But for more complex images do you think it will be a problem?\n",
    "\n",
    "Now that we’re working with image data like MNIST, we need models that can understand **spatial patterns** in images—such as edges, textures, and shapes. This is where **Convolutional Neural Networks (CNNs)** come in.\n",
    "\n",
    "---\n",
    "\n",
    "CNNs are a special class of neural networks designed specifically for processing **grid-like data**, such as images. Unlike traditional fully connected networks, CNNs take advantage of the **2D structure** of images. They can recognize patterns that occur in small regions of an image and reuse that knowledge across the whole image. They were first introduce in the 80s by [LeCun *et al.*](https://ieeexplore.ieee.org/abstract/document/6795724) alongside the `MNIST` dataset. A CNN has two main parts:\n",
    "\n",
    "<img src=\"../assets/bloc2/cnn.png\" width=\"500\" />\n",
    "\n",
    "The **convolutional** part and the classifier.\n",
    "\n",
    "## AlexNet: A Landmark in Convolutional Neural Networks\n",
    "\n",
    "**AlexNet** is a pioneering deep convolutional neural network architecture that significantly advanced the field of computer vision. Introduced by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012, it won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with a top-5 error rate of 15.3%, dramatically outperforming the previous state-of-the-art.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Architecture**: AlexNet consists of 8 learnable layers — 5 convolutional layers followed by 3 fully connected layers. Some convolutional layers are followed by **max-pooling** to reduce spatial dimensions and increase translation invariance.\n",
    "- **Activation Function**: It was one of the first deep networks to use the **ReLU (Rectified Linear Unit)** activation function, which accelerates convergence during training.\n",
    "- **Dropout**: To combat overfitting, AlexNet introduced **dropout** in the fully connected layers, a regularization technique that randomly disables neurons during training.\n",
    "- **Data Augmentation**: The network leveraged image translations and horizontal reflections to artificially expand the training dataset.\n",
    "- **GPU Training**: AlexNet was also notable for being trained on two GPUs in parallel, splitting the model to fit hardware constraints, which demonstrated the feasibility of training deep networks with high computational demand.\n",
    "\n",
    "### Impact on the Field\n",
    "\n",
    "AlexNet established **convolutional neural networks (CNNs)** as the dominant architecture for image classification and other vision tasks. Its success led to rapid developments in deeper and more efficient architectures such as VGG, GoogLeNet, and ResNet.\n",
    "\n",
    "\n",
    "### Key Building Blocks of a CNN\n",
    "\n",
    "1. **Convolutional Layers**: These layers use learnable **filters (kernels)** that slide over the input (e.g., an image) to extract local features such as edges, textures, or shapes. Each filter generates a corresponding **feature map** that highlights the presence of specific patterns across the spatial dimensions.\n",
    "\n",
    "    <img src=\"../assets/bloc2/conv.png\" width=\"500\" />\n",
    "\n",
    "   ### Key Parameters:\n",
    "   - **Filter Size (Kernel Size)**: The spatial dimensions of the filter (e.g., 3×3, 5×5). Larger filters capture broader features, while smaller filters focus on fine details.\n",
    "   - **Stride**: The number of pixels the filter moves across the input at each step. A stride of 1 means the filter slides one pixel at a time, while a stride of 2 skips every other pixel, reducing the output size.\n",
    "   - **Padding**: Determines how borders are handled.\n",
    "     - **Valid Padding**: No padding, resulting in smaller output.\n",
    "     - **Same Padding**: Pads the input so the output size matches the input size.\n",
    "   - **Number of Filters**: Determines the depth of the output feature maps. More filters capture more diverse features but increase computational cost.\n",
    "   - **Activation Function**: Typically ReLU (Rectified Linear Unit) is applied after the convolution to introduce non-linearity.\n",
    "\n",
    "   These parameters control how much spatial information is preserved, how deep and wide the resulting feature maps are, and how much computational complexity is introduced.\n",
    "\n",
    "\n",
    "2. **Activation Function (ReLU)**. Non-linear activation (usually ReLU) is applied after each convolution to introduce non-linearity and help the network learn complex patterns.\n",
    "\n",
    "3. **Pooling Layers**. These layers **downsample** the feature maps by summarizing small regions (e.g., taking the max value). Pooling helps reduce the spatial size and the number of parameters, making the model faster and more robust.\n",
    "\n",
    "4. **Classifier**. We can use any machine learning model after the convolutional part. We usually use a MLP.\n",
    "\n",
    "\n",
    "# Our first CNN\n",
    "\n",
    "We will use a more complex dataset: [Sea Animals Image Dataset](https://www.kaggle.com/datasets/vencerlanz09/sea-animals-image-dataste/data). We can find this dataset on Kaggle. Here the description:\n",
    "\n",
    "> The dataset contains different images of marine animals. Some images were taken from pixabay.com and requires no license or attribution when used. Other images were taken from flickr.com where attribution to the original authors will be required when used commercially. Currently, there are 19 different classes available and may be extended further in the future. The images are resized to either (300px, n) or (n,300px) where n is a pixel size less than 300px. See the license and attribution agreements below if you are to use the images commercially.\n",
    "\n",
    "To download the data we will use the `kagglehub` library."
   ],
   "id": "55a3886927dc5416"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"vencerlanz09/sea-animals-image-dataste\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "363ee3ebb36e5c89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will use as the `ImageFolder` class from `PyTorch` to handle the images.  from `torchvision.datasets`.\n",
    "\n",
    "#### Folder Structure\n",
    "\n",
    "The `ImageFolder` dataset assumes your images are arranged like this:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── train/\n",
    "│   ├── class1/\n",
    "│   │   ├── img001.jpg\n",
    "│   │   ├── img002.jpg\n",
    "│   ├── class2/\n",
    "│   │   ├── img001.jpg\n",
    "│   │   ├── img002.jpg\n",
    "└── val/\n",
    "    ├── class1/\n",
    "    └── class2/\n",
    "```\n",
    "\n",
    "Each **subfolder** represents a class, and each image inside is a sample of that class. The class labels are automatically inferred from the folder names.\n"
   ],
   "id": "68ecd95175bec246"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = ImageFolder(path)\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "for i, (image, label) in enumerate(dataset):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(image)\n",
    "    if i == 4:\n",
    "        break"
   ],
   "id": "4a0b9a4060c1cad8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To use the images we have to do a set of tasks:\n",
    "\n",
    "1. Divide the dataset into train/validation.\n",
    "2. Resize the images to have the same size."
   ],
   "id": "d5d91e7e2fe38406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 10\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(path, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Division\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "id": "37d05167dc3cbf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we have the data we must define the model. We will use the same approach than in the previous sessions.",
   "id": "a18fe53deeaa45f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolucional\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=25,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=25, out_channels=50, kernel_size=(3, 3), padding=\"same\"\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(in_features=32 * 32 * 50, out_features=25)  # 50\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=25, out_features=23)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        return x\n"
   ],
   "id": "19ec4c644aec3ee2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "running_loss = []\n",
    "running_acc = []\n",
    "running_test_loss = []\n",
    "running_test_acc = []\n",
    "\n",
    "for t in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    i_batch = 0\n",
    "    for i_batch, (x, y) in enumerate(tqdm(dataloader_train, desc=\"Batch\", leave=False)):  # We have to iter the batches.\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = net(x.to(device))\n",
    "\n",
    "        loss = loss_fn(y_pred, y.to(device))\n",
    "\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "        net.eval()  # Evaluation\n",
    "\n",
    "        y_pred = net(x.to(device))\n",
    "        y_pred_binary = torch.argmax(y_pred.detach().cpu(), axis=1)\n",
    "\n",
    "        batch_loss += loss_fn(y_pred, y.to(device)).detach()\n",
    "        batch_acc += accuracy_score(y_pred_binary.detach().cpu(), y.detach())\n",
    "\n",
    "    running_loss.append(batch_loss / (i_batch + 1))\n",
    "    running_acc.append(batch_acc / (i_batch + 1))\n",
    "\n",
    "    batch_test_loss = 0\n",
    "    batch_test_acc = 0\n",
    "    for i_batch, (x, y) in enumerate(dataloader_val):\n",
    "        net.eval()  # Evaluation\n",
    "\n",
    "        y_pred = net(x.to(device))\n",
    "        y_pred_binary = torch.argmax(y_pred.detach().cpu(), axis=1)\n",
    "\n",
    "        batch_test_loss += loss_fn(y_pred.detach().cpu(), y).cpu().detach()\n",
    "        batch_test_acc += accuracy_score(y_pred_binary.detach().cpu(), y.detach())\n",
    "\n",
    "    running_test_loss.append(batch_test_loss / (i_batch + 1))\n",
    "    running_test_acc.append(batch_test_acc / (i_batch + 1))"
   ],
   "id": "b6e81f59ef6523b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(running_loss, label=\"Train\")\n",
    "plt.plot(running_test_loss, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(running_acc, label=\"Train\")\n",
    "plt.plot(running_test_acc, label=\"Test\");\n",
    "plt.legend();"
   ],
   "id": "a8ce743cdac182a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<p style=\"color: green\"> Exercise: </p> How we do it to improve the model?",
   "id": "dcfaa927a27b222b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transfer learning\n",
    "\n",
    "Many authors have trained CNNs for complex tasks. If we consider the convolutional part of a CNN we can see that the features can be shared between different problems:\n",
    "\n",
    "![Features](../assets/bloc2/features.png \"CNN features\")\n",
    "\n",
    "**Transfer Learning** is a machine learning technique where a model trained on one task is reused (fully or partially) for a different but related task. It allows leveraging knowledge from large-scale pretraining to improve performance and reduce training time on smaller, domain-specific datasets.\n",
    "\n",
    "![Transfer Learning Concept](https://framerusercontent.com/images/n6FZeNKpxNGHvLDBnWU1aHvs.jpeg)\n",
    "*Image: Conceptual diagram of transfer learning*\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Pretrained Model**: A model trained on a large dataset that has already learned general features.\n",
    "- **Target Task**: The new task for which we want to adapt the pretrained model (e.g., medical image classification).\n",
    "- **Fine-Tuning**: Adjusting the weights of the pretrained model on the new dataset to specialize it.\n",
    "- **Feature Extraction**: Using the pretrained model as a fixed feature extractor (only retraining the final classifier layers).\n",
    "- **Freezing Layers**: Preventing updates to some of the pretrained layers during fine-tuning to preserve learned features.\n",
    "\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- Reduces training time and computational cost.\n",
    "- Requires less labeled data for the new task.\n",
    "- Often leads to higher accuracy, especially with small datasets.\n",
    "- Enables rapid prototyping and experimentation.\n",
    "\n",
    "## 2015\n",
    "\n",
    "In 2015, three papers were presented that marked a new breakthrough in our field. Two new architectures and a regularization technique:\n",
    "\n",
    "- **Batch Normalization**. Accelerating Deep Network Training by Reducing Internal Covariate Shift - Google [link](https://arxiv.org/pdf/1502.03167)\n",
    "- **VGG**. Very Deep Convolutional Networks for Large-Scale Image Recognition - Oxford [link](https://arxiv.org/pdf/1409.1556) - [presentation video](https://www.youtube.com/watch?v=j1jIoHN3m0s&t=31s)\n",
    "- **ResNet**. Deep Residual Learning for Image Recognition - Microsoft [link](https://arxiv.org/pdf/1512.03385)\n",
    "\n",
    "\n",
    "### VGG\n",
    "\n",
    "![VGG](https://raw.githubusercontent.com/bmalcover/aa_2425/880c98fece5e7b77dc78a95737f3509b1c084d3c/09_VGG_ResNet/img/VGG.png)\n",
    "\n",
    "One of the most significant innovations is that this network uses fixed-size kernels, unlike the variable-size kernels used in AlexNet (11×11, 5×5, 3×3). The paper argues that the ability to capture features from large kernels can be replicated by using multiple smaller kernels.\n"
   ],
   "id": "5c4f50ea0504bbc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T10:43:59.606438Z",
     "start_time": "2025-07-09T10:43:57.867564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze all convolutional layers\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier\n",
    "num_classes = 23\n",
    "\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(4096, num_classes)\n",
    ")\n",
    "\n",
    "vgg16 = vgg16.to(device)\n"
   ],
   "id": "d49d27ac09865292",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4176a0ecf3225f41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
