{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/cursSocib/blob/main/2_AA/2_1_Tabular.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "With this chapter we will start working with `PyTorch` allowing us to use more complex models, mainly deep neural networks. This is an open-source software library in the field of machine learning. It is written in Python, C++, and CUDA, and is based on the Torch software library from the LUA language. `PyTorch` was initially developed by the artificial intelligence department of the company Facebook and the company Uber.\n",
    "\n",
    "It is far more powerful than `scikit-learn` and therefore also more complex to use.\n",
    "\n",
    "### Key Differences (``PyTorch`` vs ``Scikit-learn``):\n",
    "\n",
    "| Aspect                    | PyTorch                              | Scikit-learn                                |\n",
    "|--------------------------|---------------------------------------|---------------------------------------------|\n",
    "| Flexibility              | Very high (custom models/layers)      | Limited to standard configurations          |\n",
    "| Model Definition         | Manual using classes and layers       | High-level API           |\n",
    "| Training Loop            | You write your own loop               | Handled internally                          |\n",
    "| Control over Forward Pass| Full control                          | Not customizable                            |\n",
    "| Good for                 | Research, experimentation, deep nets | Quick prototyping, small tasks              |\n",
    "\n",
    "---\n",
    "\n",
    "## A classification example\n",
    "\n",
    "We will begin to get to know this library through the execution of a dataset you already have worked with the **Titanic** dataset. You already cleaned this dataset. We will work with a clean version of it.\n"
   ],
   "id": "cb814d4ee0f8b2af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "22bbe0189a478204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.read_csv(\"../titanic/train.csv\")",
   "id": "741e70ef7a96f166"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data cleaning",
   "id": "5f21dad6625ca002"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()",
   "id": "108ac635b75b2ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we make the categorical columns *One Hot encoding* if needed.",
   "id": "3167318cfe6423c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dummy_fields=['Pclass', 'Sex', 'Embarked']\n",
    "for each in dummy_fields:\n",
    "    dummies= pd.get_dummies(df[each], prefix= each, drop_first=False)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "df.head()"
   ],
   "id": "6f90d2e2c8ba4c01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we remove all the columns that do not give us any information for the training: **Name**, **ID**, **Ticket**, and **Cabin**. And the columns that are already converted to one hot encoding.",
   "id": "3696d802d330e5a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = df.drop([\"Name\", \"PassengerId\", \"Ticket\", \"Cabin\", \"Pclass\", \"Sex\", \"Embarked\"], axis=1)",
   "id": "9da1ae5cb458c3f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "to_normalize=['Age','Fare']\n",
    "for each in to_normalize:\n",
    "    mean, std= df[each].mean(), df[each].std()\n",
    "    df.loc[:, each]=(df[each]-mean)/std\n",
    "\n",
    "df.head()"
   ],
   "id": "d04be6b4f2d70939"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We check whether there are NaNs and if that is the case we remove the rows.",
   "id": "c348ddd31ac575e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if df.isnull().values.any():\n",
    "    df = df.dropna()\n",
    "    print(\"NaNs removed\")"
   ],
   "id": "723a162ca6470bdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()",
   "id": "5ffba36df2c89de8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for series_name in df.columns:\n",
    "    if df[series_name].dtype == bool:\n",
    "        df[series_name] = df[series_name].astype('int')"
   ],
   "id": "2d1dff08686fe7bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = df\n",
    "y = df.pop(\"Survived\")\n",
    "\n",
    "# Important to use a pytorch model we need to work with Tensor\n",
    "\n",
    "X_tensor = torch.Tensor(X.values)\n",
    "y_tensor = torch.Tensor(y.values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.33, random_state=42)"
   ],
   "id": "8910696c6361378e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a model with Pytorch\n",
    "\n",
    "Neural networks for classification problems used many layers. However, for tabular data we can use a simple model with only a type of layer and an activation function.\n",
    "\n",
    "### Dense (Fully Connected) Layers\n",
    "\n",
    "A **dense layer**—also called a **fully connected (FC) layer**—is a fundamental building block in neural networks. It performs a linear transformation of the input data.\n",
    "\n",
    "Mathematically, a dense layer computes:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ is the input vector (e.g., a flattened image),\n",
    "- $\\mathbf{W}$ is the weight matrix (learned parameters),\n",
    "- $\\mathbf{b}$ is the bias vector (also learned),\n",
    "- $\\mathbf{y}$ is the output vector.\n",
    "\n",
    "Each neuron in a dense layer is connected to **every** neuron in the previous layer, hence the term *fully connected*.\n",
    "\n",
    "---\n",
    "\n",
    "### ReLU Activation Function\n",
    "\n",
    "After applying a dense layer, we often apply an **activation function** to introduce non-linearity. One of the most commonly used is the **ReLU (Rectified Linear Unit)**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "That is:\n",
    "- If the input $ z > 0 $, output is $z$,\n",
    "- If the input $ z \\leq $, output is $ 0$.\n",
    "\n",
    "---\n",
    "\n",
    "### Sigmoid or not sigmoid\n",
    "\n",
    "In neural networks, the final layer often has a specific purpose depending on the task. For **binary classification**, we typically use a **Sigmoid activation function** in the **last layer**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "That is:\n",
    "- It maps the raw output (logits) into a value between **0 and 1**,\n",
    "- The result can be interpreted as a **probability** of the positive class.\n",
    "\n",
    "We use **Sigmoid in the last layer** because:\n",
    "- It provides a **probabilistic output**, which is ideal when deciding between two classes,\n",
    "- It compresses arbitrary input values into a normalized scale.\n",
    "\n",
    "However:\n",
    "- It should be avoided in multi-class classification tasks — use **Softmax** instead,\n",
    "- If using `nn.BCEWithLogitsLoss`, you **should not apply Sigmoid manually**, as the function includes it internally for numerical stability.\n",
    "\n",
    "\n",
    "### Pytorch implementation\n",
    "\n",
    "In PyTorch, we define neural networks by subclassing `nn.Module`, which is the base class for all neural network modules. Each layer of the model is declared in the `__init__` method, and the forward computation is defined in the `forward` method.\n",
    "\n",
    "Below is an example of a simple Multilayer Perceptron (MLP) with one hidden layer:\n",
    "\n",
    "\n",
    "- `nn.Linear(in_features, out_features)` defines a fully connected (dense) layer.\n",
    "- `F.relu(...)` applies the ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "This structure is flexible and allows you to create any custom architecture by chaining layers and operations in `forward()`.\n",
    "\n",
    "- **Exercise**: We need to define the input and output of each layer. How we do it?\n"
   ],
   "id": "dd0e978371af4470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(12, 30)\n",
    "        self.fc2 = nn.Linear(30, 30)\n",
    "        self.fc3 = nn.Linear(30, 1)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Apply ReLU activation to hidden layer output\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x) # Output layer (raw logits)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return x"
   ],
   "id": "bd21ffcd2f3d5dba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we have defined the model we can instantiate. The resulting object will can be called directly, this operation will execute the `forward` method.",
   "id": "69feb2b71fef3d4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "net = MLP()",
   "id": "dd62740f8dddedc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we have a first model, we have the data, but how we train the model?\n",
    "\n",
    "## Theoretical Summary: How to Train an MLP Model with PyTorch\n",
    "\n",
    "Training a neural network like an MLP involves a few essential steps: forward pass, loss computation, backpropagation, and parameter updates. PyTorch provides a flexible and modular way to handle this process.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 1. Choose a Loss Function\n",
    "\n",
    "The **loss function** measures the difference between the model's predictions and the true labels.\n",
    "\n",
    "For classification tasks:\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Select an Optimizer\n",
    "\n",
    "An **optimizer** updates the model’s parameters using the gradients computed during backpropagation.\n",
    "\n",
    "Example using stochastic gradient descent (SGD):\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "You could also use more advanced optimizers like **Adam**:\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Training Loop\n",
    "\n",
    "This is the heart of the training process. For each epoch (complete pass through the dataset), you perform:\n",
    "\n",
    "#### a. Forward Pass\n",
    "Compute the model's predictions:\n",
    "```python\n",
    "outputs = model(inputs)\n",
    "```\n",
    "\n",
    "#### b. Loss Computation\n",
    "Calculate how wrong the predictions are:\n",
    "```python\n",
    "loss = criterion(outputs, targets)\n",
    "```\n",
    "\n",
    "Below is a table summarizing some of the most commonly used loss functions in PyTorch, along with brief verbatim explanations of what they do.\n",
    "\n",
    "| Loss Function | Description |\n",
    "|---------------|-------------|\n",
    "| `nn.MSELoss` | Measures the mean squared error (squared L2 norm) between each element in the input and target. |\n",
    "| `nn.L1Loss` | Measures the mean absolute error (L1 distance) between each element in the input and target. |\n",
    "| **`nn.CrossEntropyLoss`** | Combines `LogSoftmax` and `NLLLoss` in one single class. Useful for multi-class classification. |\n",
    "| `nn.NLLLoss` | The negative log likelihood loss. Used in conjunction with `log_softmax` output. |\n",
    "| `nn.BCELoss` | Binary Cross Entropy loss. Used for binary classification tasks. |\n",
    "| **`nn.BCEWithLogitsLoss`** | Combines a `Sigmoid` layer and the `BCELoss` in one class. More numerically stable. |\n",
    "| `nn.HingeEmbeddingLoss` | Measures whether inputs are similar or dissimilar using a margin-based criterion. |\n",
    "| `nn.MarginRankingLoss` | Encourages a distance margin between ranked inputs. Useful for pairwise ranking tasks. |\n",
    "| `nn.HuberLoss` | Combines advantages of `L1Loss` and `MSELoss`, less sensitive to outliers. |\n",
    "| `nn.SmoothL1Loss` | A smooth version of L1 loss, often used in regression tasks such as object detection. |\n",
    "| `nn.KLDivLoss` | Kullback-Leibler divergence loss, useful for comparing probability distributions. |\n",
    "\n",
    "#### c. Backward Pass\n",
    "Compute gradients of the loss with respect to model parameters:\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "#### d. Parameter Update\n",
    "Apply the gradients to update the model's parameters:\n",
    "```python\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "#### e. Zero Gradients\n",
    "Clear previous gradients before the next iteration:\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "Full loop example:\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Evaluation\n",
    "\n",
    "After training, you should evaluate the model on a validation or test set using `torch.no_grad()` to disable gradient tracking:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_inputs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Step               | Purpose                                           |\n",
    "|--------------------|---------------------------------------------------|\n",
    "| Loss Function      | Measure prediction error                          |\n",
    "| Optimizer          | Update weights using gradients                    |\n",
    "| Forward Pass       | Predict outputs                                   |\n",
    "| Loss + Backward    | Compute gradients                                 |\n",
    "| Optimizer Step     | Adjust weights                                    |\n",
    "| Zero Grad          | Prevent gradient accumulation                     |\n",
    "| Evaluation         | Test performance without gradient tracking        |\n",
    "\n",
    "This process is repeated over multiple **epochs**, gradually improving the model’s performance on the task.\n",
    "\n",
    "\n",
    "Now we make our first training. We will use the titanic data, the simple model we have defined."
   ],
   "id": "def9d9d4a953bed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "net = MLP()\n",
    "\n",
    "LR = 1e-3\n",
    "EPOCHS = 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "criterion = nn.BCELoss()"
   ],
   "id": "a4a7a23dbbbb9268"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Loop\n",
    "\n",
    "### V1"
   ],
   "id": "465e81c858f90a32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "net.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    output = net(X_train)\n",
    "    loss = criterion(output, y_train.reshape(-1, 1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = accuracy_score(y_train, (output.detach() > 0.5).int())\n",
    "\n",
    "    print(f\"Accuracy: {acc}\")"
   ],
   "id": "e6118bd7821c083d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### V2",
   "id": "b7167f37581bfb5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "net.train()\n",
    "\n",
    "accuracies = []\n",
    "for epoch in range(EPOCHS):\n",
    "    output = net(X_train)\n",
    "    loss = criterion(output, y_train.reshape(-1, 1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = accuracy_score(y_train, (output.detach() > 0.5).int())\n",
    "\n",
    "    net.eval()\n",
    "    output_val = net(X_test)\n",
    "\n",
    "    acc_val = accuracy_score(y_test, (output_val.detach() > 0.5).int())\n",
    "    accuracies.append(acc_val)\n",
    "\n",
    "    print(f\"Accuracy: {acc} - {acc_val}\")"
   ],
   "id": "1034e754b0158737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(accuracies);"
   ],
   "id": "83656a0ad8e0e128"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### V3",
   "id": "a429453f95e31af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "\n",
    "best_acc = 0\n",
    "best_weights = None\n",
    "\n",
    "losses = []\n",
    "accuracy = []\n",
    "for epoch in range(EPOCHS):\n",
    "    net.train()\n",
    "    output = net(X_train)\n",
    "    loss = criterion(output, y_train.reshape(-1, 1))\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    output_val = net(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_train, (output.detach() > 0.5).int())\n",
    "    acc_val = accuracy_score(y_test, (output_val.detach() > 0.5).int())\n",
    "\n",
    "    if acc_val > best_acc:\n",
    "        best_acc = acc\n",
    "        best_weights = copy.deepcopy(net.state_dict())\n",
    "\n",
    "    accuracy.append(acc_val)\n",
    "net.load_state_dict(best_weights);"
   ],
   "id": "234f8b95a363a7a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(losses)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(accuracy);"
   ],
   "id": "18117d32f940e37b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Regression\n",
    "\n",
    "Until now, we have worked on a **binary classification** task — predicting a category (class) based on input data. Another widely used type of supervised learning is **regression**, where the goal is to predict a **continuous numeric value** instead of a class label.\n",
    "\n",
    "**So, what changes do we need to make to adapt our classification model for regression?**\n",
    "\n",
    "First of all we obtained data for a regression problem: [Wine Quality](https://archive.ics.uci.edu/dataset/186/wine+quality)\n",
    "\n",
    "Info obtained from their website:\n",
    "\n",
    ">  Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).\n"
   ],
   "id": "a13ad81ae3b7b76c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ],
   "id": "421d94aec9b2ff43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we want to see whether the data is balanced.",
   "id": "e04b4150bc69c191"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.hist(y_train)\n",
    "plt.show()"
   ],
   "id": "9112a5cc7a9d6bc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Now you do it?\n",
    "\n",
    "What we have to change?"
   ],
   "id": "9a2945686ec88229"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
