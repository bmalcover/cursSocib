{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87562676d325070",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/cursSocib/blob/main/2_AA/2_3_Detection_segmentacton.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Detection and Segmentation\n",
    "\n",
    "## Object Detection\n",
    "\n",
    "Object detection is the task of identifying and localizing objects within an image, typically by predicting bounding boxes and class labels for each detected object. The standard pipeline involves proposing candidate bounding boxes, classifying the content within each box, and post-processing to remove redundant detections.\n",
    "\n",
    "\n",
    "<img src=\"../assets/bloc2/detection.png \" width=\"550\" />\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "Methods for object detection generally fall into either **neural network-based** or **non-neural** approaches.\n",
    "\n",
    "### Non-Neural Approaches\n",
    "\n",
    "In non-neural approaches, it is necessary to first define features using one of the methods below. After extracting features, typically using a sliding window, and then those are classified using ML classifiers. Basically the detection problem is transformed into a image classification problem.\n",
    "\n",
    "Most known techniques are:\n",
    "- Violaâ€“Jones object detection framework (based on [Haar features](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)).  \n",
    "- Scale-Invariant Feature Transform (**SIFT** [link](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html)).\n",
    "- Histogram of Oriented Gradients (**HOG** [link](https://learnopencv.com/histogram-of-oriented-gradients/)).\n",
    "\n",
    "### Neural Network Approaches\n",
    "\n",
    "Neural techniques allow for **end-to-end object detection** without explicitly defining features. These methods are typically based on **Convolutional Neural Networks (CNNs)**.\n",
    "\n",
    "- OverFeat\n",
    "- RetinaNet\n",
    "- Single Shot MultiBox Detector (SSD)\n",
    "- **Region Proposals**: R-CNN , Fast R-CNN, Faster R-CNN...\n",
    "- **You Only Look Once (YOLO)**. \n",
    "\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "1. Intersection over Union (IoU): Measures the overlap between predicted and ground truth bounding boxes. A detection is considered correct if IoU is greater than a threshold (commonly 0.5).\n",
    "\n",
    "  $$ \n",
    "  IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} \n",
    "  $$\n",
    "\n",
    "\n",
    "2. **Precision and Recall**: These metrics are used to evaluate classification performance. They are typically visualized using a **precision-recall curve**.\n",
    "\n",
    "- **Precision:**\n",
    "\n",
    "  $$ \n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} \n",
    "  $$\n",
    "\n",
    "- **Recall:**\n",
    "\n",
    "  $$ \n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} \n",
    "  $$\n",
    "\n",
    "3. **Average Precision (AP):** To resume the information given by the precision-recall curve, AP is calculated for a given class. It summarizes the precision-recall trade-off across all confidence thresholds. Often computed at different IoU thresholds (e.g., 0.5, 0.75).\n",
    "\n",
    "4. **Mean Average Precision (mAP)**: is the mean of AP values across all object classes. Multiple benchmarks use its own format, some examples:\n",
    "  - **mAP@0.5**: Mean AP at IoU threshold 0.5 (used in [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)).\n",
    "  - **mAP@0.5:0.95**: Mean AP at IoU thresholds from 0.5 to 0.95 in steps of 0.05 (used in [COCO](https://cocodataset.org/#home)).\n",
    "\n",
    "\n",
    "5. **F1 Score**: The harmonic mean of precision and recall. Less commonly used than AP/mAP in object detection benchmarks.\n",
    "\n",
    "  $$ \n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18725c51-4252-4ecb-b7e2-4b40c5ae0a08",
   "metadata": {},
   "source": [
    "We will now perform an object detection process on the image using Faster R-CNN, a detection network that outputs a label, a confidence score, and a bounding box location for each detected object.\n",
    "\n",
    "Pytorch provides the TorchVision module with differnt pretrained models [link](https://docs.pytorch.org/vision/stable/models.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731527f9-4e1f-47e6-bf09-860f33657503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "image_path = # Load the image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = F.to_tensor(image)\n",
    "\n",
    "# Load pre-trained Faster R-CNN model from torchvision\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "predictions = model([image_tensor])[0]\n",
    "predictions; # remove the ; to see the structure of a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a629b0-b04d-4a63-8aef-d713c5587522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw results\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.imshow(image)\n",
    "\n",
    "threshold =  # Change the threshold to see how it changes\n",
    "for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n",
    "    \n",
    "    if score >= threshold:\n",
    "        \n",
    "        label = label.detach().numpy()\n",
    "        box = box.detach().numpy()\n",
    "        score = score.detach().numpy()\n",
    "        \n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 10, f\"{label.item()} ({score:.2f})\",\n",
    "                color='red', fontsize=12, backgroundcolor='white')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1d9e6d99bb5b2",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "Image segmentation is a technique that divides an image into different regions to facilitate its analysis. Given an input image, the output of a segmentation process is a new image where each pixel is assigned a class.\n",
    "\n",
    "There are different types of image segmentation:\n",
    "\n",
    "### Semantic Segmentation\n",
    "\n",
    "In semantic segmentation, groups of pixels in an image are identified and classified by assigning a class label based on their characteristics such as color, texture, and shape. This provides a pixel-wise map of an image (segmentation map) to enable more detailed and accurate image analysis. Typically, one or more object classes present in the dataset are selected, and all pixels that do not belong to relevant classes are labeled as \"background.\"\n",
    "\n",
    "For example, if you want to segment trees in an image, all pixels related to a \"tree\" would be labeled with the same object name, without distinguishing between individual trees. Another example would be labeling a group of people in an image as a single object called \"people.\"\n",
    "\n",
    "### Instance Segmentation\n",
    "\n",
    "Instance segmentation is more sophisticated, as it involves identifying and delineating each individual object within an image. Therefore, it goes beyond identifying object classes and also outlines the exact boundaries of each individual object instance. Like in semantic segmentation, all pixels that do not belong to relevant classes are labeled as \"background.\"\n",
    "\n",
    "This technique must distinguish between separate objects of the same class. For example, if there are many cats in an image, instance segmentation would identify each specific cat. The segmentation map is created for every individual pixel, and separate labels are assigned to specific object instances by creating distinct labels to represent each \"cat\" in the group.\n",
    "\n",
    "![segmentation](../assets/bloc2/segmentacio_semantica.png \" Instance Segmentation\")\n",
    "\n",
    "### **Panoptic Segmentation**\n",
    "\n",
    "Panoptic segmentation goes a step further by combining the features and processes of both semantic and instance segmentation techniques. Thus, the panoptic segmentation algorithm produces a complete analysis of the image by simultaneously classifying each pixel and identifying different object instances of the same class.\n",
    "\n",
    "For example, given an image with several cars and pedestrians at a traffic signal, panoptic segmentation would label all \"pedestrians\" and \"cars\" (semantic segmentation), and also classify other elements of the scene such as traffic signs, lights, buildings, and background. In this way, panoptic segmentation detects and interprets all elements within a given image.\n",
    "\n",
    "![segmentation](../assets/bloc2/panoptic.png \" PanopticSegmentation\")\n",
    "\n",
    "### State of the Art\n",
    "\n",
    "\n",
    "_Recent_ advances rely heavily on deep learning, especially convolutional neural networks (CNNs). Notable models include:\n",
    "\n",
    "- **U-Net**: Widely used in biomedical imaging, it introduced a symmetric encoder-decoder architecture with skip connections.\n",
    "- **Mask R-CNN**: Extends Faster R-CNN by adding a branch for predicting segmentation masks for each detected object instance.\n",
    "- **DeepLab series**: Utilizes atrous (dilated) convolutions and conditional random fields to refine segmentations, achieving strong performance in semantic tasks.\n",
    "- **YOLO**: can be seen as a Swiss Army knife among vision models, capable of handling multiple tasks such as detection, segmentation, and classification.\n",
    "- **Transformer-based models** like **Segment Anything Model (SAM)** from Meta: Represent a shift toward attention mechanisms, showing state-of-the-art performance on complex datasets.\n",
    "\n",
    "\n",
    "\n",
    "Pytorch Documentation [link](https://pytorch.org/vision/main/models.html#semantic-segmentation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
