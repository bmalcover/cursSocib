{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Detection and Segmentation\n",
    "\n",
    "## Object Detection\n",
    "\n",
    "Object detection is the task of identifying and localizing objects within an image, typically by predicting bounding boxes and class labels for each detected object. The standard pipeline involves proposing candidate bounding boxes, classifying the content within each box, and post-processing to remove redundant detections.\n",
    "\n",
    "\n",
    "<img src=\"assets/bloc2/detection.png \" width=\"750\" />\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "Methods for object detection generally fall into either **neural network-based** or **non-neural** approaches.\n",
    "\n",
    "### Non-Neural Approaches\n",
    "\n",
    "In non-neural approaches, it is necessary to first define features using one of the methods below. After extracting features, typically using a sliding window, and then those are classified using ML classifiers. Basically the detection problem is transformed into a image classification problem.\n",
    "\n",
    "Most known techniques are:\n",
    "- Violaâ€“Jones object detection framework (based on Haar features).  \n",
    "- Scale-Invariant Feature Transform (**SIFT**).\n",
    "- Histogram of Oriented Gradients (**HOG**).\n",
    "\n",
    "### Neural Network Approaches\n",
    "\n",
    "Neural techniques allow for **end-to-end object detection** without explicitly defining features. These methods are typically based on **Convolutional Neural Networks (CNNs)**.\n",
    "\n",
    "- OverFeat.\n",
    "- RetinaNet.\n",
    "- **Region Proposals**: R-CNN , Fast R-CNN, Faster R-CNN.  \n",
    "- **You Only Look Once (YOLO)**. \n",
    "- Single Shot MultiBox Detector (SSD) \n",
    "\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "1. Intersection over Union (IoU): Measures the overlap between predicted and ground truth bounding boxes. A detection is considered correct if IoU is greater than a threshold (commonly 0.5).\n",
    "\n",
    "  $$ \n",
    "  IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} \n",
    "  $$\n",
    "\n",
    "\n",
    "2. **Precision and Recall**: These metrics are used to evaluate classification performance. They are typically visualized using a **precision-recall curve**.\n",
    "\n",
    "- **Precision:**\n",
    "\n",
    "  $$ \n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} \n",
    "  $$\n",
    "\n",
    "- **Recall:**\n",
    "\n",
    "  $$ \n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} \n",
    "  $$\n",
    "\n",
    "3. **Average Precision (AP):** To resume the information given by the precision-recall curve, AP is calculated for a given class. It summarizes the precision-recall trade-off across all confidence thresholds. Often computed at different IoU thresholds (e.g., 0.5, 0.75).\n",
    "\n",
    "4. **Mean Average Precision (mAP)**: is the mean of AP values across all object classes. Multiple benchmarks use its own format, some examples:\n",
    "  - **mAP@0.5**: Mean AP at IoU threshold 0.5 (used in [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)).\n",
    "  - **mAP@0.5:0.95**: Mean AP at IoU thresholds from 0.5 to 0.95 in steps of 0.05 (used in [COCO](https://cocodataset.org/#home)).\n",
    "\n",
    "\n",
    "5. **F1 Score**: The harmonic mean of precision and recall. Less commonly used than AP/mAP in object detection benchmarks.\n",
    "\n",
    "  $$ \n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "c87562676d325070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Segmentation\n",
    "\n",
    "Image segmentation is a technique that divides an image into different regions to facilitate its analysis. Given an input image, the output of a segmentation process is a new image where each pixel is assigned a class.\n",
    "\n",
    "There are different types of image segmentation:\n",
    "\n",
    "### Semantic Segmentation\n",
    "\n",
    "In semantic segmentation, groups of pixels in an image are identified and classified by assigning a class label based on their characteristics such as color, texture, and shape. This provides a pixel-wise map of an image (segmentation map) to enable more detailed and accurate image analysis. Typically, one or more object classes present in the dataset are selected, and all pixels that do not belong to relevant classes are labeled as \"background.\"\n",
    "\n",
    "For example, if you want to segment trees in an image, all pixels related to a \"tree\" would be labeled with the same object name, without distinguishing between individual trees. Another example would be labeling a group of people in an image as a single object called \"people.\"\n",
    "\n",
    "### Instance Segmentation\n",
    "\n",
    "Instance segmentation is more sophisticated, as it involves identifying and delineating each individual object within an image. Therefore, it goes beyond identifying object classes and also outlines the exact boundaries of each individual object instance. Like in semantic segmentation, all pixels that do not belong to relevant classes are labeled as \"background.\"\n",
    "\n",
    "This technique must distinguish between separate objects of the same class. For example, if there are many cats in an image, instance segmentation would identify each specific cat. The segmentation map is created for every individual pixel, and separate labels are assigned to specific object instances by creating distinct labels to represent each \"cat\" in the group.\n",
    "\n",
    "![segmentation](assets/bloc2/segmentacio_semantica.png \" Instance Segmentation\")\n",
    "\n",
    "### **Panoptic Segmentation**\n",
    "\n",
    "Panoptic segmentation goes a step further by combining the features and processes of both semantic and instance segmentation techniques. Thus, the panoptic segmentation algorithm produces a complete analysis of the image by simultaneously classifying each pixel and identifying different object instances of the same class.\n",
    "\n",
    "For example, given an image with several cars and pedestrians at a traffic signal, panoptic segmentation would label all \"pedestrians\" and \"cars\" (semantic segmentation), and also classify other elements of the scene such as traffic signs, lights, buildings, and background. In this way, panoptic segmentation detects and interprets all elements within a given image.\n",
    "\n",
    "![segmentation](assets/bloc2/panoptic.png \" PanopticSegmentation\")\n",
    "\n",
    "### State of the Art\n",
    "\n",
    "TODOOOOOOOOOOOOOO\n",
    "\n",
    "\n",
    "There are many neural networks capable of performing image segmentation. During the end of this semester, we will look at four different ones, chosen for different reasons: some for their practical utility, like **UNet** and **YOLO**; others for their historical importance in AI, like **Mask-RCNN**; and finally **SAM**, which currently represents the state-of-the-art in this task.  **DeepLabV3**\n",
    "\n",
    "Documentation [link](https://pytorch.org/vision/main/models.html#semantic-segmentation)."
   ],
   "id": "a9a1d9e6d99bb5b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
